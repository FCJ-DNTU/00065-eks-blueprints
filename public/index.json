[
{
	"uri": "//localhost:1313/5-deploymentpipeline/5.1-createacluster/",
	"title": "Create Cluster",
	"tags": [],
	"description": "",
	"content": "Create Cluster In this section, we will deploy our first EKS cluster using the eks-blueprints package. Blueprints published as npm module\nYou can learn more about Amazon EKS Blueprints for CDK\nWe edit the main file of lib/my-eks-blueprints-stack.ts:\nOpen the file lib/my-eks-blueprints-stack.ts See the sample code in the file Complete the lib/my-eks-blueprints-stack.ts file by pasting (replacing) the following code into the file: // lib/my-eks-blueprints-stack.ts\rimport * as cdk from \u0026#39;aws-cdk-lib\u0026#39;;\rimport { Construct } from \u0026#39;constructs\u0026#39;;\rimport * as blueprints from \u0026#39;@aws-quickstart/eks-blueprints\u0026#39;;\rimport { KubernetesVersion } from \u0026#39;aws-cdk-lib/aws-eks\u0026#39;;\rexport default class ClusterConstruct extends Construct {\rconstructor(scope: Construct, id: string, props?: cdk.StackProps) {\rsuper(scope, id);\rconst account = props?.env?.account!;\rconst region = props?.env?.region!;\rconst blueprint = blueprints.EksBlueprint.builder()\r.account(account)\r.region(region)\r.clusterProvider(\rnew blueprints.GenericClusterProvider({\rversion: \u0026#39;auto\u0026#39;\r})\r)\r.addOns()\r.teams()\r.build(scope, id + \u0026#34;-stack\u0026#34;);\r}\r} Open the file bin/my-eks-blueprints.ts to review the sample code. In this file, we create a CDK Construct, which is a building block of CDK representing what is necessary to create components of AWS Cloud.\nIn our case, the component is an EKS cluster blueprint placed in provided account, region, add-ons, teams (which we haven\u0026rsquo;t assigned yet) and all other resources necessary to create the blueprint (e.g., VPC, subnet, etc.). The build() command at the end initializes the cluster blueprint.\nTo actually make a construct usable in a CDK project, we need to add it to our entrypoint.\nReplace the contents of bin/my-eks-blueprints.ts with the following code block.\n// bin/my-eks-blueprints.ts\rimport * as cdk from \u0026#39;aws-cdk-lib\u0026#39;;\rimport ClusterConstruct from \u0026#39;../lib/my-eks-blueprints-stack\u0026#39;;\rimport * as dotenv from \u0026#39;dotenv\u0026#39;;\rconst app = new cdk.App();\rconst account = process.env.CDK_DEFAULT_ACCOUNT!;\rconst region = process.env.CDK_DEFAULT_REGION;\rconst env = { account, region }\rnew ClusterConstruct(app, \u0026#39;cluster\u0026#39;, { env }); Create a new .env file. Add environment variables: CDK_DEFAULT_ACCOUNT=XXXXX\rCDK_DEFAULT_REGION=XXXX Please replace CDK_DEFAULT_ACCOUNT and CDK_DEFAULT_REGION with your own values.\nImport Construct to make it available, then use the CDK app to initialize a new object of the CDK Construct we imported. Check CDK: cdk list If there are no issues, you should see the following result: cluster-stack As you can see, we can leverage EksBlueprint to define our cluster easily using CDK.\nInstead of deploying a single cluster, we will utilize the blueprint generator to add a deployment pipeline that can handle all updates for our infrastructure across different environments.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createvpcec2/",
	"title": "Create VPC và EC2 Instance",
	"tags": [],
	"description": "",
	"content": "Create a VPC Access AWS Management Console: Search for VPC. Select Create VPC. VPC Settings: In the Resources to create section, select VPC and more. In Name tag auto-generation, enter EKS Blueprint VPC. In IPv4 CIDR block, enter 10.0.0.0/16. Select Availability Zones (AZs): Choose the AZs as shown in the image and click Create VPC. Completion: After creation, you will have a VPC that looks like this. Create an EC2 Instance Access AWS Management Console: Search for EC2. Select Launch Instance. Launch an Instance: In the Name and tags section, enter EKS Blueprint Instance. Select AMI: In the Application and OS Images (Amazon Machine Image) section, choose Amazon Linux 2023 AMI. Instance Type and Key Pair: Choose t3.small. Create a key pair and name it kp-eks-blueprint. Network Settings: Select the VPC you just created. Choose public-subnet-1. Enable Auto-assign public IP. Create a Security Group. Configure Storage: Change the storage size to 30GB and click Launch Instance. Completion: You have successfully created an EC2 Instance. "
},
{
	"uri": "//localhost:1313/7-add-ons/7.1-intro/",
	"title": "Introducing add-ons",
	"tags": [],
	"description": "",
	"content": " Adding an add-on to a template is as simple as adding the .addOns method to blueprints.EksBlueprint.builder(). We will use Cluster Autoscaler as an example to show how simple add-ons are using EKS Blueprint. Add Cluster Autoscaler to your lib/pipeline.ts template as shown below: // lib/pipeline.ts\rimport * as cdk from \u0026#39;aws-cdk-lib\u0026#39;;\rimport { Construct } from \u0026#39;constructs\u0026#39;;\rimport * as blueprints from \u0026#39;@aws-quickstart/eks-blueprints\u0026#39;;\rimport { KubernetesVersion } from \u0026#39;aws-cdk-lib/aws-eks\u0026#39;;\rimport { TeamApplication, TeamPlatform } from \u0026#39;../teams\u0026#39;;\rexport default class PipelineConstruct extends Construct {\rconstructor(scope: Construct, id: string, props?: cdk.StackProps) {\rsuper(scope, id)\rconst account = props?.env?.account!;\rconst region = props?.env?.region!;\rconst blueprint = blueprints.EksBlueprint.builder()\r.account(account)\r.region(region)\r.clusterProvider(\rnew blueprints.GenericClusterProvider({\rversion: KubernetesVersion.V1_29,\r})\r)\r.addOns(new blueprints.ClusterAutoScalerAddOn) // Cluster Autoscaler addon goes here\r.teams(new TeamPlatform(account), new TeamApplication(\u0026#39;burnham\u0026#39;, account));\rblueprints.CodePipelineStack.builder()\r.name(\u0026#34;eks-blueprints-workshop-pipeline\u0026#34;)\r.owner(\u0026#34;your-github-username\u0026#34;)\r.repository({\rrepoUrl: \u0026#39;your-repo-name\u0026#39;,\rcredentialsSecretName: \u0026#39;github-token\u0026#39;,\rtargetRevision: \u0026#39;main\u0026#39;\r})\r// WE ADD THE STAGES IN WAVE FROM THE PREVIOUS CODE\r.wave({\rid: \u0026#34;envs\u0026#34;,\rstages: [\r{ id: \u0026#34;dev\u0026#34;, stackBuilder: blueprint.clone(\u0026#39;ap-southeast-1\u0026#39;) }\r]\r})\r.build(scope, id + \u0026#39;-stack\u0026#39;, props);\r}\r} If you are new to Cluster Autoscaler, this is a tool that automatically adjusts the number of nodes in your cluster when pods fail due to insufficient resources or pods are rescheduled to other nodes due to failure. used up over a long period. Push your changes to your GitHub repo to start the process. git add .\rgit commit -m \u0026#34;adding CA\u0026#34;\rgit push https://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@github.com/AWS-First-Cloud-Journey/my-eks-blueprints.git Wait about 15 minutes to complete. Then run the following command to check Cluster Autoscaler is running kubectl get pods -n kube-system "
},
{
	"uri": "//localhost:1313/8-deploy/8.1-argocd/",
	"title": "Introducing ArgoCD",
	"tags": [],
	"description": "",
	"content": "About ArgoCD The next step is to use ArgoCD to increase our team’s workload. There are two ways to leverage ArgoCD with EKS Blueprints:\nIntegrate manual workload onboarding using the ArgoCD CLI and expose the local ArgoCD server to gain access to the dashboard Leverage automated bootstrapping to automate your workload integration.\nArgoCD is a GitOps declarative, continuous delivery tool for Kubernetes. Provisions add Argo CDs to an EKS cluster and optionally launch your workloads from public and private Git repositories.\nThe Argo CD add-on allows platform administrators to combine cluster provisioning and workload bootstrapping in a single step and enables use cases such as cloning an existing running production cluster in another area for a few minutes. This is critical for business continuity and disaster recovery scenarios as well as availability across regions and geographic expansion.\nArgoCD for EKS Blueprints ArgoCD aligns well with the principles that define the value proposition of using the EKS Blueprint, including:\nApplication definitions, configurations, and environments must be declared and version controlled Application deployment and lifecycle management should be automated, testable, and easy to understand Follow the GitOps model of using Git repositories as a truism to define your desired application state Flexibility in how Kubernetes manifests are defined and managed Argo CD automates the deployment of desired application states in specified target environments. Application deployments can track updates to branches, tags, or be pinned to a specific manifest version at a Git commit. Argo CD is implemented as a Kubernetes controller that continuously monitors running applications and compares the current, live state with the desired target state (as specified in the Git repo). Deployed applications whose state directly deviates from the target state is considered out of sync. Argo CD reports \u0026amp; visualizes discrepancies, and provides means to automatically or manually synchronize live state back to the desired target state. Any modifications made to the desired target state in the Git repository can be automatically applied and reflected in the specified target environments.\nArgoCD Bootstrapping EKS Blueprints provides a bootstrap workload approach and add-ons from the customer GitOps repository.\nYou can see more documentation Cluster Bootstrapping\nTo enable bootstrapping, the ArgoCD add-on allows passing an ApplicationRepository at build time. Currently, support the following types of repositories:\nPublic HTTP/HTTPS repository (eg GitHub) Git repository accessing Private HTTPS requires username/password authentication. Private git repository with SSH access requires an SSH key for authentication. Private HTTPS accessible GitHub repository is accessible with GitHub token. "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Overview The lab uses templates and code duplication is subject to errors. Refer to Repository to edit if you get an error\nEKS Blueprints EKS Blueprints is an open-source development framework that captures the complexity of cloud infrastructure from developers and enables them to deploy workloads with ease.\nContainerized environments on AWS include many open-source or AWS products and services, including services for running containers, CI/CD pipeline, logging/metrics, and security enforcements.\nThe EKS Blueprints framework packages these tools into a cohesive whole and makes them available to development teams as a service. From an operational perspective, the framework allows companies to unify tools and best practices for securing, scaling, monitoring, and operating container infrastructure into one central platform that developers in an enterprise can use.\nWork EKS Blueprints is built on top of Amazon EKS and all the different components. EKS Blueprints are defined through Infrastructure-as-Code best practices through the AWS CDK.\nSee more documentation on EKS blueprints for CDK built with AWS CDK making it easy for customers to build and deploy EKS blueprints on Amazon EKS.\nAWS Cloud Development Kit (AWS CDK) is an open-source software development framework for defining cloud application resources in programming languages and familiar programs.\nBenefit Customers can leverage EKS Blueprints to:\nDeploy EKS clusters on any number of accounts and regions following best practices.\nManage cluster configuration, including add-ons that run in each cluster, from a single Git repository.\nDefine groups, their namespaces, and associated access permissions for your groups.\nCreate Continuous Delivery (CD) pipelines responsible for deploying your infrastructure. Leverage GitOps-based workflows to introduce and manage workloads to your team.\nConstructs of EKS Blueprints:\nBottlerocket AWS Fargate Multi-region deployments Multi-team deployments Custom cluster deployments "
},
{
	"uri": "//localhost:1313/",
	"title": "Session Management",
	"tags": [],
	"description": "",
	"content": "Introduction to EKS Blueprints Architecture Diagram Core Concepts Concepts Description Cluster An EKS Cluster deployed following best practices. Resource Provider Resource providers are abstractions that supply external AWS resources to the cluster (e.g., hosted zones, VPCs, etc.). Add-on Allows you to configure, deploy, and update the operational software or add-ons that provide key functionality to support your Kubernetes applications. Teams A logical grouping of IAM identities that have access to Kubernetes namespaces or cluster administrative access depending upon the team type. Pipelines Continuous Delivery pipelines for deploying clusters and add-ons Application An application that runs within an EKS Cluster. Blueprint EKS Blueprints allow you to configure and deploy what is known as a blueprint cluster. A blueprint combines clusters, add-ons, and teams into a cohesive object that can be deployed as a whole. Once the blueprint is configured, it can be easily deployed across any number of AWS accounts and regions. Blueprints also leverage GitOps tools to facilitate cluster bootstrapping and workload integration.\nContents Introduction Preparation Steps Creating EKS Blueprints Creating CDK Project Deploying Pipeline Onboarding Teams Add-ons Deployment Cleanup Resources "
},
{
	"uri": "//localhost:1313/6-onboardteams/6.1-definingteams/",
	"title": "Setting up teams",
	"tags": [],
	"description": "",
	"content": "Set groups Implement creating folders as teams including application and platform. mkdir teams \u0026amp;\u0026amp; cd teams \u0026amp;\u0026amp; mkdir platform-team \u0026amp;\u0026amp; mkdir application-team We’ll start by creating an IAM user for platform. aws iam create-user --user-name platform Create a file index.ts, used to create resources for platform-team cd platform-team \u0026amp;\u0026amp; touch index.ts c)\nNext we add the following code block to index.ts import { ArnPrincipal } from \u0026#34;aws-cdk-lib/aws-iam\u0026#34;;\rimport { PlatformTeam } from \u0026#39;@aws-quickstart/eks-blueprints\u0026#39;;\rexport class TeamPlatform extends PlatformTeam {\rconstructor(accountID: string) {\rsuper({\rname: \u0026#34;platform\u0026#34;,\rusers: [new ArnPrincipal(`arn:aws:iam::${accountID}:user/platform`)]\r})\r}\r} Explanation of the code block:\nThe above code block imports ArnPrincipal construct from aws-cdk-lib/aws-iam module for AWS CDK so that users can be added to the platform with IAM credentials their.\nThe best way is to extend a class using PlatformTeam class so that our platform/infrastucture people can manage users/roles, while developers can simply create groups using the provided arugments transmisson.\nThen we pass in two arguments: name and list of IAM users.\nApplication Team Create IAM user for the application team. aws iam create-user --user-name application Change directory path and create file index.ts cd ../application-team \u0026amp;\u0026amp; touch index.ts Add code to teams/application-team/index.ts file import { ArnPrincipal } from \u0026#39;aws-cdk-lib/aws-iam\u0026#39;;\rimport { ApplicationTeam } from \u0026#39;@aws-quickstart/eks-blueprints\u0026#39;;\rexport class TeamApplication extends ApplicationTeam {\rconstructor(name: string, accountID: string) {\rsuper({\rname: name, users: [new ArnPrincipal(`arn:aws:iam::${accountID}:user/application`)] });\r}\r} The Application Team template will do the following things:\nCreate a namespace Register quotas Register as an IAM user to access multiple accounts Create a shared role to access the cluster. Alternatively, an existing role can be provisioned. Register the role/user provided in the awsAuth map for kubectl and dashboard access to the cluster and namespace. We will create an additional file index.ts in the team folder cd .. \u0026amp;\u0026amp; touch index.ts In the file index.ts add the following code: export { TeamPlatform } from \u0026#39;./platform-team\u0026#39;;\rexport { TeamApplication } from \u0026#39;./application-team\u0026#39;; "
},
{
	"uri": "//localhost:1313/6-onboardteams/6.2-onboardingteams/",
	"title": "Configuring teams",
	"tags": [],
	"description": "",
	"content": "Configure groups In the previous section, we created both Applications and Platform team templates.\nAdd the following code to the template // lib/pipeline.ts // lib/pipeline.ts\rimport * as cdk from \u0026#39;aws-cdk-lib\u0026#39;;\rimport { Construct } from \u0026#39;constructs\u0026#39;;\rimport * as blueprints from \u0026#39;@aws-quickstart/eks-blueprints\u0026#39;;\rimport { KubernetesVersion } from \u0026#39;aws-cdk-lib/aws-eks\u0026#39;;\rimport { TeamPlatform, TeamApplication } from \u0026#39;../teams\u0026#39;; // HERE WE IMPORT TEAMS\rexport default class PipelineConstruct extends Construct {\rconstructor(scope: Construct, id: string, props?: cdk.StackProps) {\rsuper(scope, id)\rconst account = props?.env?.account!;\rconst region = props?.env?.region!;\rconst blueprint = blueprints.EksBlueprint.builder()\r.account(account)\r.region(region)\r.clusterProvider(\rnew blueprints.GenericClusterProvider({\rversion: \u0026#39;auto\u0026#39;,\r})\r)\r.addOns()\r.teams(new TeamPlatform(account), new TeamApplication(\u0026#39;burnham\u0026#39;,account)); // HERE WE USE TEAMS\rblueprints.CodePipelineStack.builder()\r.name(\u0026#34;eks-blueprints-workshop-pipeline\u0026#34;)\r.owner(\u0026#34;your-github-username\u0026#34;)\r.repository({\rrepoUrl: \u0026#39;your-repo-name\u0026#39;,\rcredentialsSecretName: \u0026#39;github-token\u0026#39;,\rtargetRevision: \u0026#39;main\u0026#39;\r})\r.wave({\rid: \u0026#34;envs\u0026#34;,\rstages: [\r{ id: \u0026#34;dev\u0026#34;, stackBuilder: blueprint.clone(\u0026#39;ap-southeast-1\u0026#39;) }\r]\r})\r.build(scope, id + \u0026#39;-stack\u0026#39;, props);\r}\r} Push changes to remote repository Github cd ..\rgit add .\rgit commit -m \u0026#34;adding teams\u0026#34;\rgit push https://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@github.com/AWS-First-Cloud-Journey/my-eks-blueprints.git Wait about 15 minutes for Succeeded Successfully deployed Perform test\nkubectl get ns You will notice that team-burnham is in namespace "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-connectec2/",
	"title": "Connect SSH from Visual Studio Code to EC2 Instance",
	"tags": [],
	"description": "",
	"content": "Connect SSH from Visual Studio Code to EC2 Instance Connecting SSH from Visual Studio Code to an EC2 Instance is a quick alternative to using Cloud9.\nDownload Visual Studio Code and the Remote - SSH extension: You can download VS Code here: Download VSCode.\nAfter downloading, install the following extension:\nOpen the SSH connection prompt: Click on the icon in the lower-left corner of the screen, and a dialog box will appear.\nConnect to Host: Click on Connect to Host.\nAdd New SSH Host: Click on Add New SSH Host.\nInput SSH Host Name: In the input box, enter eks-blueprint-remote and press Enter.\nConfigure SSH in config file: Click on the path in C:\\Users\\ADMIN.ssh\\config to configure it. Update SSH configuration: In the newly configured SSH block, update the information with the correct IPv4 address of the EC2 Instance and the path to the Key Pair on your machine.\nConnect to EC2: Click on the SSH icon at the bottom left corner and start the connection.\nSelect Continue: Choose Continue.\nSelect Linux: Click on Linux\nOpen Folder: Choose Open Folder and click OK.\nConnected Interface: This is what the interface looks like after connection. "
},
{
	"uri": "//localhost:1313/5-deploymentpipeline/5.2-accesscluster/",
	"title": "Create Pipeline",
	"tags": [],
	"description": "",
	"content": "Create Pipeline Setting up AWS Secrets Manager We will need to add GitHub Personal Access Token to AWS’s AWS Secrets Manager to take advantage of AWS CodePipeline and GitHub as our pipeline will take advantage of webhook to run successfully.\nYou can refer to more about how to create GitHub Personal Access Token\nAfter creating GitHub Personal Access Token\nWe return to VSCode Terminal Create Secret in Secrets Manager with the name eks-workshop-token aws secretsmanager create-secret --name \u0026#34;eks-workshop-token\u0026#34; --description \u0026#34;github access token\u0026#34; --secret-string \u0026#34;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\u0026#34; Note: remember to replace your secret-string with the token you created.\nWe can create a new CodePipelineStack resource by creating a new CDK Construct in the lib/ directory, then importing Construct into the main entry point file.\nCreate new construct file. touch lib/pipeline.ts Once the file is created, open the file and add the following code to create pipeline construct // lib/pipeline.ts\rimport * as cdk from \u0026#39;aws-cdk-lib\u0026#39;;\rimport { Construct } from \u0026#39;constructs\u0026#39;;\rimport * as blueprints from \u0026#39;@aws-quickstart/eks-blueprints\u0026#39;;\rimport { KubernetesVersion } from \u0026#39;aws-cdk-lib/aws-eks\u0026#39;;\rexport default class PipelineConstruct extends Construct {\rconstructor(scope: Construct, id: string, props?: cdk.StackProps) {\rsuper(scope, id)\rconst account = props?.env?.account!;\rconst region = props?.env?.region!;\rconst blueprint = blueprints.EksBlueprint.builder()\r.account(account)\r.region(region)\r.clusterProvider(\rnew blueprints.GenericClusterProvider({\rversion: \u0026#39;auto\u0026#39;\r})\r)\r.addOns()\r.teams();\rblueprints.CodePipelineStack.builder()\r.name(\u0026#34;eks-blueprints-workshop-pipeline\u0026#34;)\r.owner(\u0026#34;your-github-username\u0026#34;)\r.repository({\rrepoUrl: \u0026#39;your-repo-name\u0026#39;,\rcredentialsSecretName: \u0026#39;github-token\u0026#39;,\rtargetRevision: \u0026#39;main\u0026#39;\r})\r.build(scope, id+\u0026#39;-stack\u0026#39;, props);\r}\r} Make configuration:\nname, we enter eks-blueprints-workshop-pipeline or the name pipeline you want. owner, enter your github name. (in the lab, enter AWS-First-Cloud-Journey) repoUrl, enter the name of the repo. (In the lab, enter my-eks-blueprints) credentialsSecretName, enter your secret (In the lab, enter eks-workshop-token) targetRevision, enter revision main To make sure we can access Construct, we need to import and initialize a new construct.\nChange the content of the file bin/my-eks-blueprints.ts // bin/my-eks-blueprints.ts\r// bin/my-eks-blueprints.ts\rimport * as cdk from \u0026#39;aws-cdk-lib\u0026#39;;\rimport ClusterConstruct from \u0026#39;../lib/my-eks-blueprints-stack\u0026#39;;\rimport * as dotenv from \u0026#39;dotenv\u0026#39;;\rimport PipelineConstruct from \u0026#39;../lib/pipeline\u0026#39;; // IMPORT OUR PIPELINE CONSTRUCT\rdotenv.config();\rconst app = new cdk.App();\rconst account = process.env.CDK_DEFAULT_ACCOUNT!;\rconst region = process.env.CDK_DEFAULT_REGION;\rconst env = { account, region }\rnew ClusterConstruct(app, \u0026#39;cluster\u0026#39;, { env });\rnew PipelineConstruct(app, \u0026#39;pipeline\u0026#39;, { env }); Do a list check pipeline cdk list Do more Stage. In this step, we add stages to the pipeline (in the lab using the dev stage, you can deploy more stages for test and production in regions. other) // lib/pipeline.ts\rimport * as cdk from \u0026#39;aws-cdk-lib\u0026#39;;\rimport { Construct } from \u0026#39;constructs\u0026#39;;\rimport * as blueprints from \u0026#39;@aws-quickstart/eks-blueprints\u0026#39;;\rimport { KubernetesVersion } from \u0026#39;aws-cdk-lib/aws-eks\u0026#39;;\rexport default class PipelineConstruct extends Construct {\rconstructor(scope: Construct, id: string, props?: cdk.StackProps) {\rsuper(scope, id)\rconst account = props?.env?.account!;\rconst region = props?.env?.region!;\rconst blueprint = blueprints.EksBlueprint.builder()\r.account(account)\r.region(region)\r.clusterProvider(\rnew blueprints.GenericClusterProvider({\rversion: \u0026#39;auto\u0026#39;\r})\r)\r.addOns()\r.teams();\rblueprints.CodePipelineStack.builder()\r.name(\u0026#34;eks-blueprints-workshop-pipeline\u0026#34;)\r.owner(\u0026#34;your-github-username\u0026#34;)\r.repository({\rrepoUrl: \u0026#39;your-repo-name\u0026#39;,\rcredentialsSecretName: \u0026#39;github-token\u0026#39;,\rtargetRevision: \u0026#39;main\u0026#39;\r})\r// WE ADD THE STAGES IN WAVE FROM THE PREVIOUS CODE\r.wave({\rid: \u0026#34;envs\u0026#34;,\rstages: [\r{ id: \u0026#34;dev\u0026#34;, stackBuilder: blueprint.clone(\u0026#39;ap-southeast-1\u0026#39;) }\r]\r})\r.build(scope, id + \u0026#39;-stack\u0026#39;, props);\r}\r} Use class blueprints.StackStage build support to define our stages using .stage\nUse .wave support for parallel deployment.\nIn the lab, we are deploying a cluster.\nIf you’re deploying multiple clusters, for mitigation we’ll simply add .wave to the list of stages to include how you want to structure your different deployment stages in the pipeline. (ie different add-ons, region deployment, etc.).\nOur stack will deploy the following clusters: EKS in the dev environment. CodePipeline deploys to the region: ap-southeast-1.\nPerform pipeline list recheck cdk list The following results:\ncluster-stack\rpipeline-stack\rpipeline-stack/dev/dev-blueprint "
},
{
	"uri": "//localhost:1313/8-deploy/8.2-deploy/",
	"title": "Deploying Workload with ArgoCD",
	"tags": [],
	"description": "",
	"content": "Deploy Workload with ArgoCD Define workload repo ArgoCD Bootstrapping starts with defining variables with repo information such as the URL of the workload repo and the in-app path of the applications. We will use a smaller version of the application. Full-scale example of an application containing a workload for team-burnham:\nconst repoUrl = \u0026#39;https://github.com/aws-samples/eks-blueprints-workloads.git\u0026#39;\rconst bootstrapRepo : blueprints.ApplicationRepository = {\rrepoUrl,\rtargetRevision: \u0026#39;workshop\u0026#39;,\r} You can see more EKS Blueprints Workloads\nArgoCD add-on definition The variables can then be passed as a parameter in the ArgoCD add-on definitions for our stage. Optionally, you can set a secret for the Argo admin.\nconst prodBootstrapArgo = new blueprints.ArgoCDAddOn({\rbootstrapRepo: {\r...bootstrapRepo,\rpath: \u0026#39;envs/dev\u0026#39;\r},\r}); You can set different paths from the repo based on the environment you are working in. Since our path has dev, test, and prod deployments, we can set the path to ’envs/dev’, ’envs/test’ and ’env/prod’ and set the variables to names individual.\nWe can then pass this information to the pipeline using the addOns method as part of the stackBuilder property that drives the blueprints.\nblueprints.CodePipelineStack.builder()\r.name(\u0026#34;pipeline-name\u0026#34;)\r.owner(\u0026#34;owner-name\u0026#34;)\r.repository({\rrepoUrl: \u0026#39;repo-name\u0026#39;,\rcredentialsSecretName: \u0026#39;github-token\u0026#39;,\rtargetRevision: \u0026#39;main\u0026#39;\r})\r.wave({\rid: \u0026#39;envs\u0026#39;,\rstages: [\r{ id: \u0026#34;dev\u0026#34;, stackBuilder: blueprint.clone(\u0026#39;ap-southeast-1\u0026#39;).addOns(devBootstrapArgo)}\r]\r})\r.build(app, \u0026#39;pipeline-stack\u0026#39;); Make changes to the file lib/pipeline-stack.ts // lib/pipeline.ts\rimport * as cdk from \u0026#39;aws-cdk-lib\u0026#39;;\rimport { Construct } from \u0026#39;constructs\u0026#39;;\rimport * as blueprints from \u0026#39;@aws-quickstart/eks-blueprints\u0026#39;;\rimport { KubernetesVersion } from \u0026#39;aws-cdk-lib/aws-eks\u0026#39;;\rimport { TeamApplication, TeamPlatform } from \u0026#39;../teams\u0026#39;;\rexport default class PipelineConstruct extends Construct {\rconstructor(scope: Construct, id: string, props?: cdk.StackProps) {\rsuper(scope, id)\rconst account = props?.env?.account!;\rconst region = props?.env?.region!;\rconst blueprint = blueprints.EksBlueprint.builder()\r.account(account)\r.region(region)\r.version(\u0026#34;auto\u0026#34;)\r.addOns(\rnew blueprints.ClusterAutoScalerAddOn,\rnew blueprints.KubeviousAddOn(),\r)\r.teams(new TeamPlatform(account), new TeamApplication(\u0026#39;burnham\u0026#39;, account));\r// HERE WE ADD THE ARGOCD APP OF APPS REPO INFORMATION\rconst repoUrl = \u0026#39;https://github.com/aws-samples/eks-blueprints-workloads.git\u0026#39;;\rconst bootstrapRepo: blueprints.ApplicationRepository = {\rrepoUrl,\rtargetRevision: \u0026#39;workshop\u0026#39;,\r}\r// HERE WE GENERATE THE ADDON CONFIGURATIONS\rconst devBootstrapArgo = new blueprints.ArgoCDAddOn({\rbootstrapRepo: {\r...bootstrapRepo,\rpath: \u0026#39;envs/dev\u0026#39;\r},\r});\rblueprints.CodePipelineStack.builder()\r.name(\u0026#34;eks-blueprints-workshop-pipeline\u0026#34;)\r.owner(\u0026#34;your-github-username\u0026#34;)\r.repository({\rrepoUrl: \u0026#39;your-repo-name\u0026#39;,\rcredentialsSecretName: \u0026#39;github-token\u0026#39;,\rtargetRevision: \u0026#39;main\u0026#39;\r})\r// WE ADD THE STAGES IN WAVE FROM THE PREVIOUS CODE\r.wave({\rid: \u0026#34;envs\u0026#34;,\rstages: [\r// HERE WE ADD OUR NEW ADDON WITH THE CONFIGURED ARGO CONFIGURATIONS\r{ id: \u0026#34;dev\u0026#34;, stackBuilder: blueprint.clone(\u0026#39;ap-southeast-1\u0026#39;).addOns(devBootstrapArgo) }\r]\r})\r.build(scope, id + \u0026#39;-stack\u0026#39;, props);\r}\r} Thực hiện push thay đổi lên Github repository git add .\rgit commit -m \u0026#34;Bootstrapping ArgoCD\u0026#34;\rgit push https://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@github.com/FromSunNews/my-eks-blueprints.git Đợi 15 phút sau sẽ hoàn thành Thực hiện kiểm tra argocd namespace bằng lệnh. kubectl get ns "
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "Preparation To conduct the lab, we have to prepare the Cloud9 environment and create the IAM role for the Cloud9 instance.\nAlso, install Kubernetes Tool\nContent Create Environment Cloud9 Install Tool Create IAM role Assign IAM role to Cloud9 instance Update IAM role "
},
{
	"uri": "//localhost:1313/7-add-ons/7.2-testingcluster/",
	"title": "Testing Cluster Autoscaler",
	"tags": [],
	"description": "",
	"content": "Check Cluster Autoscaler we were able to deploy Cluster Autoscaler successfully.\nThe following steps will help test and validate the Cluster Autoscaler functionality in your cluster.\nDeploy a sample application as a deployment. Scale deployment to 50. Scaling event monitoring.\nDeploy sample application\nCheck the number of available nodes. kubectl get nodes Make sample nginx application Create a directory and a file named nginx.yaml: mkdir -p /home/ec2-user/environment\rsudo vi /home/ec2-user/environment/nginx.yaml Copy the following content into the nginx.yaml file:\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: nginx-to-scaleout\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: nginx\rtemplate:\rmetadata:\rlabels:\rservice: nginx\rapp: nginx\rspec:\rcontainers:\r- image: nginx\rname: nginx-to-scaleout\rresources:\rlimits:\rcpu: 500m\rmemory: 512Mi\rrequests:\rcpu: 500m\rmemory: 512Mi Finally, apply the nginx.yaml file:\nkubectl apply -f ~/environment/nginx.yaml Check the pod is running kubectl get pod -l app=nginx Implement Scale deployment replicas We can now scale the deployment to 10 replicas and observe the deployment: kubectl scale --replicas=10 deployment/nginx-to-scaleout Next we do Monitoring the scaling event Some pods will be in a Pending state, which will trigger the cluster-autoscaler to expand the EC2 pool: kubectl get pods -l app=nginx -o wide --watch To view the cluster-autoscaler log kubectl -n kube-system logs -f deployment/blueprints-addon-cluster-autoscaler-aws-cluster-autoscaler You can list all the nodes kubectl get nodes To delete the execution resource kubectl delete deploy nginx-to-scaleout\rrm ~/environment/nginx.yaml "
},
{
	"uri": "//localhost:1313/7-add-ons/7.3-createaddons/",
	"title": "Create add-ons",
	"tags": [],
	"description": "",
	"content": "Create add-ons First, we create kubevious_addon.ts in the lib folder touch lib/kubevious_addon.ts Add the following code to the kubevious_addon.ts file // lib/kubevious_addon.ts\rimport { Construct } from \u0026#39;constructs\u0026#39;;\rimport * as blueprints from \u0026#39;@aws-quickstart/eks-blueprints\u0026#39;;\rimport { setPath } from \u0026#39;@aws-quickstart/eks-blueprints/dist/utils/object-utils\u0026#39;;\r/**\r* User provided options for the Helm Chart\r*/\rexport interface KubeviousAddOnProps extends blueprints.HelmAddOnUserProps {\rversion?: string,\ringressEnabled?: boolean,\rkubeviousServiceType?: string,\r}\r/**\r* Default props to be used when creating the Helm chart\r*/\rconst defaultProps: blueprints.HelmAddOnProps \u0026amp; KubeviousAddOnProps = {\rname: \u0026#34;blueprints-kubevious-addon\u0026#34;,\rnamespace: \u0026#34;kubevious\u0026#34;,\rchart: \u0026#34;kubevious\u0026#34;,\rversion: \u0026#34;0.9.13\u0026#34;,\rrelease: \u0026#34;kubevious\u0026#34;,\rrepository: \u0026#34;https://helm.kubevious.io\u0026#34;,\rvalues: {},\ringressEnabled: false,\rkubeviousServiceType: \u0026#34;ClusterIP\u0026#34;,\r};\r/**\r* Main class to instantiate the Helm chart\r*/\rexport class KubeviousAddOn extends blueprints.HelmAddOn {\rreadonly options: KubeviousAddOnProps;\rconstructor(props?: KubeviousAddOnProps) {\rsuper({...defaultProps, ...props});\rthis.options = this.props as KubeviousAddOnProps;\r}\rdeploy(clusterInfo: blueprints.ClusterInfo): Promise\u0026lt;Construct\u0026gt; {\rlet values: blueprints.Values = populateValues(this.options);\rconst chart = this.addHelmChart(clusterInfo, values);\rreturn Promise.resolve(chart);\r}\r}\r/**\r* populateValues populates the appropriate values used to customize the Helm chart\r* @param helmOptions User provided values to customize the chart\r*/\rfunction populateValues(helmOptions: KubeviousAddOnProps): blueprints.Values {\rconst values = helmOptions.values ?? {};\rsetPath(values, \u0026#34;ingress.enabled\u0026#34;, helmOptions.ingressEnabled);\rsetPath(values, \u0026#34;kubevious.service.type\u0026#34;, helmOptions.kubeviousServiceType);\rsetPath(values, \u0026#34;mysql.generate_passwords\u0026#34;, true);\rreturn values;\r} Then add the following code to lib/pipeline.ts // lib/pipeline-stack.ts\rimport * as cdk from \u0026#39;aws-cdk-lib\u0026#39;;\rimport { Construct } from \u0026#39;constructs\u0026#39;;\rimport * as blueprints from \u0026#39;@aws-quickstart/eks-blueprints\u0026#39;;\rimport { TeamPlatform, TeamApplication } from \u0026#39;../teams\u0026#39;; export default class PipelineConstruct extends Construct {\rconstructor(scope: Construct, id: string, props?: cdk.StackProps){\rsuper(scope,id)\rconst blueprint = blueprints.EksBlueprint.builder()\r.account(account)\r.region(region)\r.addOns(\rnew blueprints.ClusterAutoScalerAddOn,\rnew blueprints.KubeviousAddOn(), // New addon goes here\r) .teams(new TeamPlatform(account), new TeamApplication(\u0026#39;burnham\u0026#39;,account));\rblueprints.CodePipelineStack.builder()\r.name(\u0026#34;eks-blueprints-workshop-pipeline\u0026#34;)\r.owner(\u0026#34;your-github-username\u0026#34;)\r.repository({\rrepoUrl: \u0026#39;your-repo-name\u0026#39;,\rcredentialsSecretName: \u0026#39;github-token\u0026#39;,\rtargetRevision: \u0026#39;main\u0026#39;\r})\r.wave({\rid: \u0026#34;envs\u0026#34;,\rstages: [\r{ id: \u0026#34;dev\u0026#34;, stackBuilder: blueprint.clone(\u0026#39;ap-southeast-1\u0026#39;)}\r]\r})\r.build(scope, id+\u0026#39;-stack\u0026#39;, props);\r}\r} Do push to Github repository git add .\rgit commit -m \u0026#34;adding Kubevious\u0026#34;\rgit push https://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@github.com/AWS-First-Cloud-Journey/my-eks-blueprints.git Wait 15 minutes to complete Once the pipeline is complete, we can see our add-ons in action by running the command below: kubectl port-forward $(kubectl get pods -n kubevious -l \u0026#34;app.kubernetes.io/component=kubevious-ui\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) 8080:80 -n kubevious "
},
{
	"uri": "//localhost:1313/3-createeksblueprints/",
	"title": "Create EKS Blueprints",
	"tags": [],
	"description": "",
	"content": "Create EKS Blueprints Refer to how to create Github Repository\nAccess to New repository of Github\nIn the Create a new repository interface, enter my-eks-blueprints for Repository name Select Public Select Create repository After creating repository successfully\nCopy and store HTTPS path of Git repository In the Github interface we will install and create token\nSelect Avatar of your Github account Select Settings Then scroll down and select Developer settings In the Developer settings interface\nSelect Personal access tokens Select Generate new token In the Generate new token interface\nNote, enter eks-workshop-toke Select the following scope: repo and admin:repo_hook Select Generate token 7. Select Generate token\nComplete Generate token\nCopy and store token Refer to how to create Personal Access Token\nTải git sudo dnf install git -y\rgit --version Do a clone repository git clone https://github.com/\u0026lt;your-alias\u0026gt;/my-eks-blueprints.git "
},
{
	"uri": "//localhost:1313/8-deploy/8.3-manage/",
	"title": "Manage workloads on ArgoCD",
	"tags": [],
	"description": "",
	"content": "Workload management on ArgoCD Now, let’s log into the user interface and see how the workloads are being managed through ArgoCD.\nBy default, the argocd-server service is not public. For the purpose of this workshop, we will use the Load Balancer to use. kubectl patch svc blueprints-addon-argocd-server -n argocd -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; Wait 5 minutes, LoadBalancer is created. export ARGOCD_SERVER=`kubectl get svc blueprints-addon-argocd-server -n argocd -o json | jq --raw-output \u0026#39;.status.loadBalancer.ingress[0].hostname\u0026#39;` TYPE and EXTERNAL-IP on argo server service changed to LoadBalancer. Copy the EXTERNAL-IP of LoadBalancer. kubectl get svc -n argocd Open a browser and paste the EXTERNAL-IP of LoadBalancer in. Implement automatic password generation and username is admin kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d After logging in, observe the workloads on the ArgoCD UI "
},
{
	"uri": "//localhost:1313/5-deploymentpipeline/5.3-pipelineinaction/",
	"title": "Pipeline in Action",
	"tags": [],
	"description": "",
	"content": "Pipeline in Action Constructs have been modified, then saved.\nWe do add, commit and push your changes to remote repository git add .\rgit commit -m \u0026#34;Setting up EKS Blueprints deployment pipeline\u0026#34;\rgit branch -M main\rgit config credential.helper store\rgit push https://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@github.com/AWS-First-Cloud-Journey/my-eks-blueprints.git Since this is your first time pushing to Github’s repomte repository, Cloud 9 will prompt you to enter your GitHub credentials. You will need to use your GitHub password (if 2FA is not enabled) or your Github Token (if 2FA is enabled). In the lab, we use Github Token because using user name and password is no longer valid.\nIf you forgot Secret you can view it in AWS Secret Manager\nThe credential.helper call is used to store your credentials so you don’t have to keep entering them every time you make a change.\nNote: git push uses the accompanying token https://[token]@github.com/[github_name]/[repo_name].git\nCheck if the repository has been pushed up yet? After pushing to the repository, we deploy the pipeline stack. cdk deploy pipeline-stack You will be prompted to confirm the pipeline stack deployment.\nType y and then press enter. After successful deployment will display Stack ARN Return to the AWS Management Console interface Find and select CodePipeline You will see the rollout in progress. Wait about 30 minutes, Pipeline shows Succeeced\nCodePipeline will pick up the changes made in the remote repository and pipelne will start building. Updates (add, remove, fix code) can be seen in the CodePipeline Console to verify that the stages are built correctly.\nSelect the pipeline name.\nSee Source and Build steps\nSource: Source stage runs an action to retrieve code changes when the pipeline is run manually or when a webhook event is sent from the source provider. In our case, every time we make a code change in our my-eks-blueprints repository and reflect the changes in the remote repo, the event will be sent to the pipeline (with GitHub personal access token) ) to enable new pipeline execution. Build: build stage allows you to run test and build actions as part of the pipeline. During Build, the pipeline runs scripts to make sure everything works as intended. This includes npm package installations, version checking and CDK synth. Any error in the configuration from your repo may make this stage fail. You can see a list of commands run in this action by clicking Details in actions (below its name and AWS Codebuild). Followed by UpdatePipeline and Assets\nUpdatePipeline: This is an extra build stage that runs to check if the pipeline needs updating. For example, if the code is changed to include additional (out-of-production) stages, UpdatePipeline will run the build and reconfigure pipeline that needs to add those additional stages. This stage is the Assets needed to run the stages. Assets: This is a series of build actions that handle the assets needed to deploy the EKS cluster. Asset, in the context of CDK, are local files, directories, or Docker images that can be packaged into CDK libraries and applications. These assets or artifacts are necessary for our CDK application to function. These assets allow the Framework to work properly, as they contain the parameters and configurations used to deploy the necessary resources i.e. Cluster Provider, Kubernetes resources in Cluster, IAM, add-ons with Helm Charts, etc. Assets are stored on AWS as Lambda Functions for S3 Artifacts bucket stored files and executables. Finally dev (Prepare and Deploy) * **Envs (our wave)**: a wave is an implementation option for pipelines that provide multiple stages (or environments) in parallel. Because the CDK aggregates code into a CloudFormation template, you can view it in the stack deployment management console as a CloudFormation template.\rIf you encounter an error during the pipeline execution, click to view the details. This error indicates that the queue limit has been exceeded. You can try retry it. And finally, it should run successfully. "
},
{
	"uri": "//localhost:1313/6-onboardteams/6.3-clusteraccessforteams/",
	"title": "Team Access",
	"tags": [],
	"description": "",
	"content": "Group Access Burnham Team, only having access to resources in their dedicated namespace along with a demonstration of how we can use Kubernative native construct to ensure that only people used in team-burnham namespace can access those resources. This is also known as soft multi-tenancy you are using Kubernetes constructs like namespaces, quotas, and network policies to prevent applications from being accessed. implementations in different namespaces communicate with each other. kubectl describe role -n team-burnham You can see that Team Burnham can only get and list a set of application-focused Kubernetes resources (pods, daemonsets, deployments, replicasets, statefulsets, and jobs). You’ll notice that they don’t have permission to create or delete resources in their respective namespaces.\nRetrieve the created role for Team burnham by running the following command: aws cloudformation describe-stacks --stack-name dev-dev-blueprint | jq -r \u0026#39;.Stacks[0].Outputs[] | select(.OutputKey|match(\u0026#34;burnhamteamrole\u0026#34;))| .OutputValue\u0026#39; Create credentials for application aws iam create-login-profile --user-name application --password Ekscdkworkshop123! Go to AWS\nPerform login with IAM user Enter your Account ID Select Next Next,\nEnter IAM user name as application Enter password just created Select Sign in Complete the login In the AWS interface\nSelect Switch role In the Switch Role interface\nAccount, enter your Account ID Then enter Role Select Switch Role Complete Switch Role Access to EKS Here you will see an error message stating that the Team Burnham user is NOT allowed to list deployments in all namespaces. When you select team-burnham in namespace, you will see the forbidden message disappear. This means that you are currently showing Team Burnham workloads (no workloads since any workloads have not been deployed). "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.3-installtool/",
	"title": "Tool Installation",
	"tags": [],
	"description": "",
	"content": "Installing kubectl Amazon EKS clusters require the kubectl, kubelet, and aws-cli or aws-iam-authenticator tools to enable IAM authentication for your Kubernetes cluster.\nInstall kubectl by using the following commands: sudo curl --silent --location -o /usr/local/bin/kubectl \\\rhttps://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/kubectl\rsudo chmod +x /usr/local/bin/kubectl For more information, refer to the official AWS guide for installing kubectl.\nUpdate awscli with the following commands: curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34;\runzip awscliv2.zip\rsudo ./aws/install Verify the installation by running the following command: for command in kubectl jq envsubst aws\rdo\rwhich $command \u0026amp;\u0026gt;/dev/null \u0026amp;\u0026amp; echo \u0026#34;$command in path\u0026#34; || echo \u0026#34;$command NOT FOUND\u0026#34;\rdone Enable kubectl bash completion with the following commands: kubectl completion bash \u0026gt;\u0026gt; ~/.bash_completion\r. /etc/profile.d/bash_completion.sh\r. ~/.bash_completion "
},
{
	"uri": "//localhost:1313/5-deploymentpipeline/5.4-accessingthecluster/",
	"title": "Access Cluster",
	"tags": [],
	"description": "",
	"content": "Cluster Access Install access to cluster export KUBE_CONFIG=$(aws cloudformation describe-stacks --stack-name dev-dev-blueprint | jq -r \u0026#39;.Stacks[0].Outputs[] | select(.OutputKey|match(\u0026#34;ConfigCommand\u0026#34;)))| .OutputValue \u0026#39;)\r$KUBE_CONFIG Once kubeconfig has been updated, you will be able to access the EKS cluster kubectl get svc "
},
{
	"uri": "//localhost:1313/4-createcdkproject/",
	"title": "Create CDK Project",
	"tags": [],
	"description": "",
	"content": "Create CDK Project Change the directory to the main repo and install nvm cd my-eks-blueprints\rcurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash\rexport NVM_DIR=\u0026#34;$([ -z \u0026#34;${XDG_CONFIG_HOME-}\u0026#34; ] \u0026amp;\u0026amp; printf %s \u0026#34;${HOME}/.nvm\u0026#34; || printf %s \u0026#34;${XDG_CONFIG_HOME}/nvm\u0026#34;)\u0026#34;\r[ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ] \u0026amp;\u0026amp; \\. \u0026#34;$NVM_DIR/nvm.sh\u0026#34;\rsource ~/.bashrc\rnvm -v Use Node.js version 18 nvm install v18\rnvm use v18\rnode -v\rnpm -v You need to use Node.js version 14.15.0 or higher to use CDK. For more information, see here Install TypeScript and CDK version 2.147.3 npm -g install typescript\rnpm install -g aws-cdk@2.147.3\rcdk --version Initialize a new CDK project using TypeScript cdk init app --language typescript In the VSCode interface View the sidebar Examine the structure of the project lib/: This is where the stacks or constructs of your CDK project are defined. bin/my-eks-blueprints.ts: This is the entry point of the CDK project. It will load the constructs defined in lib/. You can read more about CDK.\nSet the AWS_DEFAULT_REGION and ACCOUNT_ID export AWS_DEFAULT_REGION=ap-southeast-1\rexport ACCOUNT_ID=212454837823 Note: Remember to replace ACCOUNT_ID with your actual ID for the lab.\nInitialize the bootstrap account To perform bootstrapping, run:\ncdk bootstrap --trust=$ACCOUNT_ID \\\r--cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\raws://$ACCOUNT_ID/$AWS_REGION On successful bootstrapping, you will see:\nEnvironment aws://212454837823/ap-southeast-1 bootstrapped. Install the eks-blueprints and dotenv modules for the project npm i @aws-quickstart/eks-blueprints dotenv "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.4-createrole/",
	"title": "Create IAM Role",
	"tags": [],
	"description": "",
	"content": "Creating IAM Role for Cloud9 Instance First, access the AWS Management Console Search for and select IAM In the IAM interface Select Roles Click on Create role In the Select trusted entity step Choose AWS service Select EC2 Click Next In the Add permission step Search for AdministratorAccess Select AdministratorAccess Click Next Complete the Name section For Name, enter eks-blueprints-cdk-workshop-admin Click Create role You have successfully created an IAM role for the EC2 Instance "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.5-attachrole/",
	"title": "Attach IAM role",
	"tags": [],
	"description": "",
	"content": "Attach IAM Role to Cloud9 Instance First, access the AWS Management Console\nSearch for EC2 and select Instance In the EC2 instance interface\nSelect EKS Blueprint Instance Choose Actions Select Security Click on Modify IAM role In the Modify IAM role interface\nChoose the eks-blueprints-cdk-workshop-admin role Click Update IAM role "
},
{
	"uri": "//localhost:1313/5-deploymentpipeline/",
	"title": "Build Deployment Pipeline",
	"tags": [],
	"description": "",
	"content": "Building Deployment Pipeline In this section, we’ll look at how to set up a deployment pipeline to automate updates for our cluster. While it’s convenient to leverage the CDK command-line tool to deploy your first stack, it’s a good idea to set up automated pipelines responsible for deploying and updating your EKS infrastructure. We will use CodePipelineStack of Framework to deploy environments in different regions.\nCodePipelineStack is a structure for easy continuous delivery of AWS CDK applications. Whenever you check out the source code of an AWS CDK application on GitHub, the stack can automatically build, test, and deploy your new version.\nCodePipelineStack updates itself: if you add stages or application stacks, the pipeline will automatically reconfigure itself to deploy those new stages or stacks.\nContent Create Cluster Create Pipeline Pipeline in Action Cluster Access "
},
{
	"uri": "//localhost:1313/6-onboardteams/",
	"title": "Manage teams using IaC",
	"tags": [],
	"description": "",
	"content": "OnboardTeams In this section, we will introduce our teams to EKS Blueprints. we’ll look at how to join a Platform team and an Application team and make sure we’re defining the right access levels for our teams. An example would be that our Application team must have read-only access to the underlying infrastructure and must be extended to their namespace while our Platform team will have granular levels of access. because the Platform team will be responsible for managing the underlying infrastructure.\nBenefits of managing teams using infrastructure as code (IaC):\nSelf-documenting code Focused logic related to the group Ability to use repeatable templates to create new environments. Content OnboardTeams Referral group Cluster Access "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.6-updaterole/",
	"title": "Update IAM Role",
	"tags": [],
	"description": "",
	"content": "Update IAM Role Configure the environment export ACCOUNT_ID=xxx\rexport AWS_REGION=xxx Check the AWS_REGION test -n \u0026#34;$AWS_REGION\u0026#34; \u0026amp;\u0026amp; echo AWS_REGION is \u0026#34;$AWS_REGION\u0026#34; || echo AWS_REGION is not set Save to bash_profile echo \u0026#34;export ACCOUNT_ID=${ACCOUNT_ID}\u0026#34; | tee -a ~/.bash_profile\recho \u0026#34;export AWS_REGION=${AWS_REGION}\u0026#34; | tee -a ~/.bash_profile\raws configure set default.region ${AWS_REGION}\raws configure get default.region Verify IAM Role aws sts get-caller-identity --query Arn | grep eks-blueprints-cdk-workshop-admin -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34; If the result is \u0026ldquo;IAM role NOT valid,\u0026rdquo; please review the previous steps to ensure that the IAM role you created and assigned to the Cloud9 Workspace is correct.\n"
},
{
	"uri": "//localhost:1313/7-add-ons/",
	"title": "Add-ons",
	"tags": [],
	"description": "",
	"content": "Add-ons Add-ons are third-party (and native AWS) solutions that provide the functionality needed to optimize the efficient running of EKS Blueprints. Add-ons allow you to configure the tools and services you want to run to support your EKS workload. When you configure add-ons for a blueprint, the add-ons are made available at deployment time. Add-ons can deploy both Kubernetes-specific and AWS resources needed to support the add-on functionality.\nThe benefit of leveraging the EKS Blueprints Add-on is that you extend your ability to leverage open-source projects and tools built by the Kubernetes community. These projects and tools address different areas of running your workload on Kubernetes such as security, observability, CI/CD, GitOps, and more.\nAdd-on Description AppMesh Adds an AppMesh controller and CRDs. ArgoCD Provisions Argo CD into your cluster. AWS Load Balancer Controlle Provisions the AWS Load Balancer Controller into your cluster Calico Adds the Calico 1.7.1 CNI/Network policy engine. Cluster Autoscaler Adds the standard cluster autoscaler. Container Insights Adds Container Insights support integrating monitoring with CloudWatch. CoreDNS Adds CoreDNS (flexible, extensible DNS server) Amazon EKS add-on. ExternalDNS Adds External DNS support for AWS to the cluster, integrating with Amazon Route 53 Kube Proxy Adds kube-proxy Amazon EKS add-on (maintains network rules on each Amazon EC2 node). Metrics Server Adds metrics server (pre-req for HPA and other monitoring tools). Nginx Adds NGINX ingress controller. Secrets Store Adds AWS Secrets Manager and Config Provider for Secret Store CSI Driver to the EKS Cluster. SSM Agent Adds Amazon SSM Agent to worker nodes. VPC CNI Adds the Amazon VPC CNI Amazon EKS addon to support native VPC networking. Weave GitOps Weave GitOps Core AddOn. X-Ray Adds XRay Daemon to the EKS Cluster. OPA Gatekeeper Adds policy management features to your cluster Velero Adds Velero to the EKS Cluster. Content Introducing add-ons Test Cluster Autotscaler Create add-ons "
},
{
	"uri": "//localhost:1313/8-deploy/",
	"title": "Deploying Workload with ArgoCD",
	"tags": [],
	"description": "",
	"content": "Deploy Workload with ArgoCD You have now successfully provisioned an EKS cluster with your teams, roles, and pipelines to automate and manage the infrastructure. The next step is to leverage GitOps methodologies using ArgoCD to manage and automate our application workloads using technologies and tools you are probably already familiar with like Git.\nIn this section, we will demonstrate how to leverage ArgoCD to deploy and manage our application workloads.\nWhat is ArgoCD? How to set up and deploy our workload with ArgoCD. Use the ArgoCD user interface to manage deployed workloads. It is important to understand the key principles of GitOps before diving into this section. Key Principles of GitOps GitOps is declarative which means that a system managed with GitOps must have its desired state expressed declaratively. The desired state is stored in a way that enforces immutability, versioning, and restoring a complete version history. The software agent pulls the desired state declarations from the source. Softare employees continuously observe the actual system state and try to apply the desired state. Content About ArgoCD Deploy Workload with ArgoCD Workload Management on ArgoCD "
},
{
	"uri": "//localhost:1313/9-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "Clean up resources Perform a delete EKS Blueprints cd ~/environment/my-eks-blueprints\rcdk destroy --all Select y Go to CloudFormation console Select the Stack to delete. Then select Delete "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]